### 🔹 Basic Kafka Interview Questions

1. What is Apache Kafka?
- Apache Kafka is a distributed event streaming platform used to build real-time data pipelines and streaming applications. 
- It is designed to handle high-throughput, low-latency, fault-tolerant data transmission across distributed systems.
- Apache Kafka is like a messaging system where different applications can send (produce) and receive (consume) data (messages) in real-time — much like a publish-subscribe system.

Core Features
1.High throughput – Handles millions of messages per second.
2.Scalability – Horizontally scalable by adding brokers.
3.Durability – Data is persisted to disk.
4.Fault tolerance – Replication across brokers ensures availability.
5.Real-time processing – Supports stream processing via Kafka Streams.

Key Components
1.Producer – Sends (publishes) messages to Kafka topics.
2.Consumer – Reads messages from Kafka topics.
3.Topic – A logical channel to which producers send and consumers read messages.
4.Broker – A Kafka server that stores and manages messages.
5.Partition – A topic is divided into partitions for scalability and parallelism.
6.Zookeeper – Coordinates the Kafka brokers (metadata, leader election). (Optional in newer versions with KRaft mode)

Typical Use Cases
1.Real-time analytics and monitoring (e.g., website activity tracking)
2.Log aggregation (e.g., centralized log systems)
3.Event sourcing and CQRS
4.Messaging backbone between microservices
5.Stream processing pipelines (via Kafka Streams or Apache Flink)

----------------------------------------------------------------------------------------------------------------------------------------------
2. What are the main components of Kafka?
1. Producer
* Sends (publishes) data (messages/events) to Kafka topics.
* Pushes data to a specific topic.
* Can choose the partition based on a key (for ordering) or let Kafka decide.
* Can be asynchronous or synchronous.

2. Consumer
* Reads data from Kafka topics.
* Works in consumer groups to enable parallel processing.
* Each partition is read by only one consumer in a group (load balancing).
* Tracks its position using offsets.

3. Topic
* A named stream of records/messages.
* Data is categorized and stored in topics.
* Topics are divided into partitions for scalability and parallelism.

4. Partition
* Each topic is split into one or more partitions.
* Messages within a partition are ordered.
* Allows Kafka to scale horizontally across brokers.
* Partitions are the unit of parallelism in Kafka.

5. Broker
* A Kafka server that stores data and serves clients.
* Each broker handles multiple topic-partitions.
* A Kafka cluster consists of multiple brokers.
* One broker acts as a leader for a partition; others are replicas.

6. Zookeeper (Deprecated in newer versions, replaced by KRaft mode)
* Used for cluster coordination, configuration, and leader election.
* Manages metadata and broker health.
* In Kafka 2.8+ and later, KRaft mode (Kafka Raft) removes dependency on Zookeeper.

7. Consumer Group
* A group of consumers working together to consume messages from a topic.
* Ensures scalable and fault-tolerant consumption.
* Each partition is assigned to only one consumer within the group.

8. Leader and Follower (Replication)
* Each partition has one leader and multiple followers.
* The leader handles all reads and writes.
* Followers replicate the data for high availability.

----------------------------------------------------------------------------------------------------------------------------------------------
3. What is a Kafka topic and partition?
A Kafka topic is like a category or feed name to which records (messages) are sent by producers and from which they are read by consumers.
* Think of it as a channel where messages are published and subscribed.
* You can have multiple producers and multiple consumers for a single topic.
Example: A topic named `"user-signups"` could store all new user registration events.

What is a Partition?
A partition is a sub-division of a topic. Each topic can be split into multiple partitions to allow parallelism and scalability.

Each partition:
* Is an ordered sequence of messages.
* Stores messages with an offset (unique ID).
* Is stored on a Kafka broker.
* Has one leader (for read/write) and zero or more replicas (for fault tolerance).

Messages within a partition are always in the same order they were written.
Example:
Suppose you have a topic:
Topic: orders
Partitions: 3 (P0, P1, P2)
Kafka will distribute incoming messages across the 3 partitions. For example:

| Message | Partition |
| ------- | --------- |
| Order1  | P0        |
| Order2  | P1        |
| Order3  | P2        |
| Order4  | P0        |

Why Use Partitions?
* Scalability: Allows Kafka to distribute load across multiple brokers.
* Parallelism: Multiple consumers can read in parallel.
* Performance: Better throughput for both producers and consumers.

Summary Table:

| Concept   | Description                           |
| --------- | ------------------------------------- |
| Topic     | Logical stream of messages            |
| Partition | Physical storage unit within a topic  |
| Offset    | Unique ID of a message in a partition |
| Broker    | Kafka server storing topic partitions |

----------------------------------------------------------------------------------------------------------------------------------------------
4. What is the role of Zookeeper in Kafka?
Role of Zookeeper in Apache Kafka (Traditional Kafka Architecture)
-Apache Kafka traditionally uses Apache Zookeeper to manage and coordinate the Kafka cluster.
-Zookeeper helps maintain cluster metadata, broker status, and configuration data.


Key Roles of Zookeeper in Kafka

1. Broker Registration and Discovery
   * Keeps track of all Kafka brokers in the cluster.
   * When a new broker starts, it registers itself with Zookeeper.

2. Leader Election
   * Helps elect a leader broker for each topic partition.
   * Ensures high availability by choosing another leader if the current one fails.

3. Configuration Management
   * Stores important configuration data (like ACLs, quotas, etc.).
   * Allows brokers and clients to fetch updated settings.

4. Cluster State Management
   * Tracks which brokers are alive and which are down.
   * Acts as the source of truth for Kafka cluster health.

5. Controller Election
   * Kafka has a controller node responsible for managing partition leaders and replicas.
   * Zookeeper helps elect and monitor this controller.

6. Topic Metadata and Quorum Management
   * Zookeeper maintains metadata like which partitions belong to which broker.
   * Assists in maintaining synchronization between brokers.


Architecture Diagram (Simplified):

Kafka Cluster
 ├── Broker 1
 ├── Broker 2
 └── Broker 3
      ↑
     [ Zookeeper ]
       (Manages brokers, leader election, configs)


Kafka without Zookeeper)

* Since Kafka 2.8, Kafka has introduced KRaft mode (Kafka Raft Metadata mode), which allows running Kafka without Zookeeper.
* Kafka now manages metadata internally using the Raft consensus algorithm.

Summary:

| Function                  | Role of Zookeeper                        |
| ------------------------- | ---------------------------------------- |
| Broker Management         | Registers, tracks, and discovers brokers |
| Leader Election           | Picks partition and controller leaders   |
| Configuration Storage     | Centralized config repository            |
| Cluster Health Monitoring | Tracks broker and controller status      |

----------------------------------------------------------------------------------------------------------------------------------------------
5. What is a Kafka producer and consumer?
What is a Kafka Producer?
A Kafka Producer is a client application that sends (publishes) data to Kafka topics.

Key Points:
* Sends records (key-value pairs) to Kafka topics.
* Can write to a specific partition using a key, or let Kafka assign one.
* Supports asynchronous (non-blocking) and synchronous (blocking) sends.
* Can configure acknowledgment levels for reliability:

  * `acks=0`: No acknowledgment.
  * `acks=1`: Leader acknowledgment.
  * `acks=all`: All in-sync replicas must acknowledge.

Example: A web app sends user activity logs to a Kafka topic using a producer.


What is a Kafka Consumer?

A Kafka Consumer is a client application that reads (subscribes to) data from Kafka topics.

Key Points:

* Reads messages from one or more topics.
* Belongs to a consumer group for load balancing.
* Each partition is read by only one consumer per group.
* Tracks read progress using offsets (can auto-commit or manually commit).

Example: A service that reads user activity logs from Kafka to analyze behavior.

Flow Summary:
[ Producer ] → sends → [ Kafka Topic (with partitions) ] → read by → [ Consumer ]

Real-World Example:
* Producer: A payment service sends transaction data.
* Kafka: Stores the transactions in a `payments` topic.
* Consumer: A fraud detection service reads transactions in real time.

Summary Table:

| Feature    | Kafka Producer           | Kafka Consumer                |
| ---------- | ------------------------ | ----------------------------- |
| Role       | Publishes data to Kafka  | Reads data from Kafka         |
| Sends to   | Topics (and partitions)  | Subscribes to topics          |
| Common Use | Logging, metrics, events | Analytics, alerts, processing |
| Offset     | Not applicable           | Tracks position in the topic  |

----------------------------------------------------------------------------------------------------------------------------------------------
6. How does Kafka ensure message durability?
Kafka ensures message durability—that is, data is not lost even if brokers crash or restart—through a combination of disk persistence,
replication, and acknowledgment settings.

1. Disk-Based Persistence
* All messages written to Kafka are immediately written to disk.
* Kafka stores data in commit logs for each partition.
* This ensures messages survive broker restarts or crashes.
> Data is stored in segment files in the broker's local file system.

2. Replication
* Kafka replicates each partition across multiple brokers (called replica brokers).
* One replica is elected as the leader; others are followers.
* Followers copy data from the leader to stay in sync.
> Example: With replication factor 3, your data exists on 3 different brokers.

3. In-Sync Replicas (ISR)
* ISR = Set of replicas that are fully caught up with the leader.
* Kafka only acknowledges writes once the message is written to all ISRs (if `acks=all` is set).
* Ensures that even if the leader fails, one of the ISR replicas can take over without data loss.

4. Producer Acknowledgments (`acks` setting)
* Controls when a producer gets a confirmation:
  * `acks=0`: No durability (no acknowledgment).
  * `acks=1`: Durability depends on the leader only.
  * `acks=all`: Safest — wait for all ISR replicas to acknowledge.

5. Log Retention and Configuration
* Kafka retains messages based on:
  * Time (`log.retention.hours`)
  * Size (`log.retention.bytes`)
* Data is retained even after it’s consumed, depending on config.

6. Write-Ahead Log (WAL) Nature
* Kafka never deletes data immediately.
* Uses an append-only log structure—great for durability and recovery.

Summary Table:

| Feature            | How It Ensures Durability                      |
| ------------------ | ---------------------------------------------- |
| Disk Storage       | All messages are written to disk               |
| Replication        | Data exists on multiple brokers                |
| In-Sync Replicas   | Ensures replicas are updated before ack        |
| acks=all           | Confirms message only when all ISRs receive it |
| Retention Settings | Controls how long data is stored               |

----------------------------------------------------------------------------------------------------------------------------------------------
7. What is the difference between Kafka and traditional messaging systems like RabbitMQ or ActiveMQ?
Apache Kafka is often compared with traditional message brokers like RabbitMQ and ActiveMQ, but they are designed with 
different use cases and architectures in mind.

High-Level Difference

| Feature                        | Kafka                                                         | RabbitMQ / ActiveMQ                                     |
| ------------------------------ | ----------------------------------------------------------------- | ----------------------------------------------------------- |
| Type                       | Distributed streaming platform                                    | Traditional message broker                                  |
| Storage Model              | Log-based (append-only, durable logs)                             | Queue-based (messages removed after read)                   |
| Message Retention          | Retains messages for a configured time even after consumption     | Deletes after consumer acknowledgment                       |
| Throughput                 | Very high (millions of messages/sec)                              | Moderate to high                                            |
| Ordering Guarantee         | Per-partition ordering                                            | Queue-level ordering (can break in load-balanced consumers) |
| Consumer Model             | Pull-based                                                        | Push-based                                                  |
| Use Case Fit               | Event streaming, real-time analytics                              | Task queues, transactional messaging                        |
| Replay Capability          | Yes (via offset seeking)                                          | No (once acknowledged, message is gone)                     |
| Scalability                | Designed for horizontal scaling (via partitions)                  | Limited scaling; clustering is harder                       |
| Durability                 | Disk-based + replicated                                           | Durable queues (can persist to disk)                        |
| Built-in Stream Processing | Yes (Kafka Streams API)                                           | No                                                          |


Use Cases Comparison

| Use Case                             | Kafka                      | RabbitMQ / ActiveMQ |
| ------------------------------------ | -------------------------- | ------------------- |
| Log aggregation                      | ✅ Excellent                | ⚠️ Not ideal        |
| Event sourcing                       | ✅ Ideal                    | ❌ Poor fit          |
| Task distribution (e.g., job queues) | ⚠️ Possible, but not ideal | ✅ Designed for it   |
| Real-time data pipeline              | ✅ Built for this           | ❌ Not optimized     |
| Message prioritization               | ❌ No built-in support      | ✅ Supported         |
| Request/Response pattern             | ❌ Not designed for it      | ✅ Supported         |

When to Use Kafka
* You need high throughput, durable logs, and message replay.
* You’re building event-driven systems or streaming pipelines.
* You want horizontal scalability with high availability.

When to Use RabbitMQ / ActiveMQ
* You need complex routing, prioritized messages, or low-latency task distribution.
* You require request/response, RPC, or transactional messaging.
* You’re building traditional work queue-based systems.
----------------------------------------------------------------------------------------------------------------------------------------------
8. How do Kafka consumers read data?
Kafka consumers are client applications that subscribe to topics and read data (messages) from Kafka partitions in a controlled 
and reliable manner.

How It Works – Step by Step
1. Subscription to Topics
   * A consumer subscribes to one or more Kafka topics.
   * Kafka assigns specific partitions of the topic to the consumer.

2. Partition Assignment
   * If the consumer is part of a consumer group, Kafka will assign each partition to only one consumer in the group.
   * Enables parallel processing and load balancing.

3. Polling for Data (Pull Model)
   * Consumers use the poll() method to pull messages from Kafka.
   * Kafka does not push data to consumers (unlike some traditional brokers).

4. Message Offset
   * Every message in a partition has a unique offset (sequence number).
   * The consumer keeps track of which offsets have been read.
   * Offset can be:
     * Auto-committed (default behavior)
     * Manually committed for more control and reliability

5. Processing Messages
   * After reading, consumers process the messages.
   * If successful, they commit the offset so they don’t re-read the same message.

6. Offset Storage
   * Kafka stores offsets in a special internal topic (`__consumer_offsets`).
   * Consumers can resume from the last committed offset even after a crash.

Example Flow
Kafka Topic: orders (3 partitions)
Consumer Group: order-processing-group
→ Consumer 1: Reads from Partition 0
→ Consumer 2: Reads from Partition 1
→ Consumer 3: Reads from Partition 2

Key Concepts
| Concept            | Description                                                                |
| ------------------ | -------------------------------------------------------------------------- |
| poll()         | Method used by consumers to fetch data from Kafka                          |
| Offset         | Unique ID of each message in a partition                                   |
| Consumer Group | A group of consumers coordinating to read from a topic collectively        |
| Rebalancing    | Happens when a new consumer joins/leaves the group (partition reassigning) |


Read Behavior Types
| Behavior          | Description                                  |
| ----------------- | -------------------------------------------- |
| At-most-once  | Read → Process → Commit (risk of loss)       |
| At-least-once | Read → Commit → Process (risk of duplicates) |
| Exactly-once  | Requires Kafka transactions + idempotence    |

----------------------------------------------------------------------------------------------------------------------------------------------
9. What is a consumer group in Kafka?
A consumer group in Kafka is a group of one or more consumers that coordinate to read data from a Kafka topic in parallel and without overlap.

Why Use Consumer Groups?
Consumer groups enable:
* Scalability — multiple consumers can read from a topic simultaneously.
* Load balancing — partitions are distributed across consumers.
* Fault tolerance — if one consumer fails, others take over its partitions.

How It Works
1. Each consumer in the group is assigned one or more partitions of the topic.
2. Each partition is consumed by only one consumer in the group at a time.
3. Multiple consumer groups can read the same data independently.

Example
* Topic: `payments` (3 partitions)
* Consumer Group: `payment-processor`
* 3 Consumers: C1, C2, C3

| Partition | Assigned Consumer |
| --------- | ----------------- |
| P0        | C1                |
| P1        | C2                |
| P2        | C3                |

> If C2 fails, Kafka rebalances the group and reassigns its partition (e.g., P1) to another active consumer.

Key Properties
| Property            | Explanation                                                    |
| ------------------- | -------------------------------------------------------------- |
| Isolation       | Each group processes data independently                        |
| Offset tracking | Kafka tracks the read position (offset) per consumer group |
| Rebalancing     | Happens when a consumer joins or leaves the group              |
| Parallelism     | You can scale processing by adding more consumers to the group |
| Auto commit     | Consumers can auto-commit offsets or manage them manually      |

Use Case Example
* Analytics app: One group reads topic `click-events` to update real-time dashboards.
* ETL app: Another group reads the same topic to store events in a data warehouse.

Each group maintains its own offset and reads independently.

Summary
| Term           | Description                                             |
| -------------- | ------------------------------------------------------- |
| Consumer Group | Set of consumers that share the work of reading a topic |
| Partition Rule | One partition → one consumer (within a group)           |
| Benefits       | Scalability, fault tolerance, isolation                 |

----------------------------------------------------------------------------------------------------------------------------------------------
10. How does Kafka handle message retention?
Kafka retains messages for a configured period of time or size, regardless of whether they’ve been consumed or not. 
This is different from traditional brokers (like RabbitMQ), where messages are deleted once consumed.

Message Retention Basics
Kafka stores messages in a log file per partition. It doesn’t delete them immediately after consumption. Instead, it retains them based on:
1. Time-Based Retention
   * Messages are kept for a set time (e.g., 7 days).
   * Controlled by:
     log.retention.hours (default: 168 hours = 7 days)

2. Size-Based Retention
   * Retains messages until the total log size exceeds a limit.
   * Controlled by:
     log.retention.bytes
     
3. Log Segment Files
   * Kafka divides the partition log into segments.
   * Older segments are deleted as per retention settings.

Retention Configuration (Per Topic or Broker Level)

| Property              | Description                              |
| --------------------- | ---------------------------------------- |
| `log.retention.hours` | How long to retain messages (in hours)   |
| `log.retention.bytes` | How much total data to retain (in bytes) |
| `log.segment.bytes`   | Size of each segment file                |
| `log.segment.ms`      | Time interval to roll over segments      |
| `delete.retention.ms` | Delay before deleting a compacted record |

Two Retention Modes
1. Delete (Default)
* Messages are deleted after they expire (based on time or size).

2. Compact (Log Compaction)
* Kafka retains only the latest value for each key.
* Useful for change logs or state storage.
* Enabled with:
  cleanup.policy=compact

Consumption is Decoupled from Retention
* Kafka does not delete messages after consumption.
* Consumers track their own offsets, and can re-read old messages as long as they haven’t been deleted by retention rules.

Example Scenario
If `log.retention.hours=168` and a message was produced on Monday at 2 PM:
* It will be stored until the following Monday at 2 PM, regardless of whether it was consumed.

Summary Table

| Retention Type | Trigger                         | Use Case                         |
| -------------- | ------------------------------- | -------------------------------- |
| Time-based     | After X hours                   | Most common (default)            |
| Size-based     | After total log exceeds X bytes | Storage-constrained environments |
| Log compaction | Keeps latest message per key    | Change data capture, caching     |
----------------------------------------------------------------------------------------------------------------------------------------------

### 🔹 Intermediate Kafka Interview Questions

1. What happens if a Kafka broker goes down?
2. What is ISR (In-Sync Replicas)?
3. How does Kafka provide fault tolerance?
4. Explain producer acknowledgment levels (acks=0, 1, all).
5. How can you achieve exactly-once message delivery in Kafka?
6. What is Kafka offset? How is it managed?
7. Difference between Kafka at-most-once, at-least-once, and exactly-once delivery semantics?
8. What is log compaction in Kafka?
9. How do Kafka partitions help with scalability?
10. How can you monitor Kafka? What tools are available?

---

### 🔹 Advanced Kafka Interview Questions

1. What is Kafka Streams? How is it different from Kafka Consumer API?
2. How does Kafka handle backpressure?
3. What are idempotent producers in Kafka?
4. What is the role of Kafka Connect?
5. How do you secure a Kafka cluster?

   * SSL, SASL, ACLs
6. What is the use of Kafka Schema Registry?
7. How does Kafka guarantee order of messages?
8. How do you implement transactional messaging in Kafka?
9. How do you perform Kafka cluster rebalancing?
10. Describe Kafka internal storage format.

---

### 🔹 Scenario-Based Kafka Questions

1. How would you design a Kafka-based system to process millions of messages per second?
2. What would you do if consumers are falling behind the producer rate?
3. How to handle message replay in Kafka?
4. Your Kafka consumer keeps getting duplicate messages — how would you debug and fix it?
5. Describe a time you handled a Kafka cluster upgrade.
